{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üíé XGBoost Sniper v3: Obsidian Refinement\n",
    "\n",
    "**Goal:** Create a hyper-refined model that limits volume to ~10 picks/day ('Sniper' approach) to maximize ROI.\n",
    "\n",
    "**Methodology:**\n",
    "1. **Fetch & Engineer:** Reuse the v2 Hybrid Momentum features.\n",
    "2. **Train:** XGBoost Classifier.\n",
    "3. **Optimize (NEW):** Instead of one pick per league, we implement a **Daily Portfolio Cap**.\n",
    "   - Sort all daily candidates by **Expected Value (Edge * Prob)**.\n",
    "   - Take the top `N` picks per day (e.g., Top 10).\n",
    "   - Use stricter value floors (e.g., only bets with >5% edge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OPTIMIZATION FOR RYZEN 7 7700 (16 Threads)\n",
    "N_JOBS = 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Fetching 'picks'... 5000... 10000... 15000... 20000... 25000... 30000... 35000... 40000... 45000... 50000... 55000... 60000... 65000... 70000... 75000... 80000... 85000... Done. (87686 rows)\n",
      "üì• Fetching 'capper_directory'... Done. (513 rows)\n",
      "üì• Fetching 'leagues'... Done. (30 rows)\n",
      "üì• Fetching 'bet_types'... Done. (13 rows)\n",
      "üîÑ Linking data relationships...\n",
      "üßπ Filtered out 780 'Lotto' plays (+250 or higher).\n"
     ]
    }
   ],
   "source": [
    "class SportsDataPipeline:\n",
    "    def __init__(self):\n",
    "        self.url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "        self.key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "        if not self.url or not self.key:\n",
    "            raise ValueError(\"‚ùå Missing Supabase credentials in .env file.\")\n",
    "        self.supabase: Client = create_client(self.url, self.key)\n",
    "    \n",
    "    def _fetch_all_batches(self, table_name, select_query=\"*\", batch_size=1000):\n",
    "        all_rows = []\n",
    "        start = 0\n",
    "        print(f\"üì• Fetching '{table_name}'...\", end=\" \", flush=True)\n",
    "        \n",
    "        while True:\n",
    "            end = start + batch_size - 1\n",
    "            try:\n",
    "                response = self.supabase.table(table_name).select(select_query).range(start, end).execute()\n",
    "                data = response.data\n",
    "                if not data: break\n",
    "                \n",
    "                all_rows.extend(data)\n",
    "                if len(all_rows) % 5000 == 0: print(f\"{len(all_rows)}...\", end=\" \", flush=True)\n",
    "                if len(data) < batch_size: break\n",
    "                start += batch_size\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error fetching batch starting at {start}: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Done. ({len(all_rows)} rows)\")\n",
    "        return all_rows\n",
    "\n",
    "    def fetch_master_data(self):\n",
    "        # 1. Fetch Picks\n",
    "        pick_cols = \"id, pick_date, pick_value, unit, odds_american, result, capper_id, league_id, bet_type_id\"\n",
    "        picks_data = self._fetch_all_batches('picks', pick_cols)\n",
    "        df_picks = pd.DataFrame(picks_data)\n",
    "        if df_picks.empty: return pd.DataFrame()\n",
    "\n",
    "        # 2. Fetch References\n",
    "        cappers = pd.DataFrame(self._fetch_all_batches('capper_directory', \"id, canonical_name\"))\n",
    "        leagues = pd.DataFrame(self._fetch_all_batches('leagues', \"id, name, sport\"))\n",
    "        bet_types = pd.DataFrame(self._fetch_all_batches('bet_types', \"id, name\"))\n",
    "        \n",
    "        # 3. Merge\n",
    "        print(\"üîÑ Linking data relationships...\")\n",
    "        df = df_picks.merge(cappers, left_on='capper_id', right_on='id', how='left', suffixes=('', '_capper'))\n",
    "        df = df.merge(leagues, left_on='league_id', right_on='id', how='left', suffixes=('', '_league'))\n",
    "        \n",
    "        if not bet_types.empty:\n",
    "            df = df.merge(bet_types, left_on='bet_type_id', right_on='id', how='left', suffixes=('', '_bt'))\n",
    "            df.rename(columns={'name_bt': 'bet_type_name'}, inplace=True)\n",
    "        else:\n",
    "            df['bet_type_name'] = 'unknown'\n",
    "\n",
    "        # 4. Cleanup\n",
    "        df['pick_date'] = pd.to_datetime(df['pick_date'])\n",
    "        if 'name' in df.columns: df.rename(columns={'name': 'league_name'}, inplace=True)\n",
    "        if 'sport' in df.columns: df.rename(columns={'sport': 'sport_name'}, inplace=True)\n",
    "        \n",
    "        # Filter Lotto Plays\n",
    "        df['odds_american'] = pd.to_numeric(df['odds_american'], errors='coerce').fillna(-110)\n",
    "        original_len = len(df)\n",
    "        df = df[df['odds_american'] <= 250] \n",
    "        print(f\"üßπ Filtered out {original_len - len(df)} 'Lotto' plays (+250 or higher).\")\n",
    "        \n",
    "        return df.sort_values('pick_date')\n",
    "\n",
    "# Load Data\n",
    "pipeline = SportsDataPipeline()\n",
    "raw_data = pipeline.fetch_master_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è  Engineering Features (V3 Logic)...\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def _american_to_decimal(self, odds):\n",
    "        if pd.isna(odds) or odds == 0: return 1.91 \n",
    "        if odds > 0: return (odds / 100) + 1\n",
    "        return (100 / abs(odds)) + 1\n",
    "\n",
    "    def _calculate_streaks(self, series):\n",
    "        streaks = series.groupby((series != series.shift()).cumsum()).cumcount() + 1\n",
    "        result_array = np.where(series == 0, -streaks, streaks)\n",
    "        return pd.Series(result_array, index=series.index)\n",
    "\n",
    "    def process_features(self):\n",
    "        print(\"üõ†Ô∏è  Engineering Features (V3 Logic)...\")\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # 1. Standard Conversion\n",
    "        df['unit'] = pd.to_numeric(df['unit'], errors='coerce').fillna(1.0)\n",
    "        df['decimal_odds'] = df['odds_american'].apply(self._american_to_decimal)\n",
    "        \n",
    "        if 'result' in df.columns:\n",
    "            res = df['result'].astype(str).str.lower().str.strip()\n",
    "            conditions = [\n",
    "                res.isin(['win', 'won', 'hit']),\n",
    "                res.isin(['loss', 'lost', 'miss']),\n",
    "                res.isin(['push', 'void', 'draw', 'tie'])\n",
    "            ]\n",
    "            df['outcome'] = np.select(conditions, [1.0, 0.0, 0.5], default=np.nan)\n",
    "        \n",
    "        conditions_roi = [df['outcome'] == 1.0, df['outcome'] == 0.0]\n",
    "        choices_roi = [df['unit'] * (df['decimal_odds'] - 1), -df['unit']]\n",
    "        df['profit_units'] = np.select(conditions_roi, choices_roi, default=0.0)\n",
    "        \n",
    "        df = df.sort_values(['capper_id', 'pick_date'])\n",
    "        \n",
    "        # 2. Rolling Stats (Capper)\n",
    "        df['capper_experience'] = df.groupby('capper_id').cumcount()\n",
    "        \n",
    "        df = df.set_index('pick_date')\n",
    "        grouped = df.groupby('capper_id')\n",
    "        for window in ['7D', '30D']:\n",
    "            s = window.lower()\n",
    "            df[f'acc_{s}'] = grouped['outcome'].transform(lambda x: x.rolling(window, min_periods=1).mean().shift(1))\n",
    "            df[f'roi_{s}'] = grouped['profit_units'].transform(lambda x: x.rolling(window, min_periods=1).sum().shift(1))\n",
    "            df[f'vol_{s}'] = grouped['profit_units'].transform(lambda x: x.rolling(window, min_periods=1).std().shift(1))\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # 3. Hotness & Streaks\n",
    "        df['raw_hotness'] = df.groupby('capper_id')['profit_units']\\\n",
    "            .transform(lambda x: x.ewm(span=10, adjust=False).mean().shift(1))\n",
    "            \n",
    "        df['prev_outcome_binary'] = (df['outcome'] == 1.0).astype(int)\n",
    "        df['streak_entering_game'] = df.groupby('capper_id')['prev_outcome_binary']\\\n",
    "            .transform(lambda x: self._calculate_streaks(x).shift(1))\n",
    "\n",
    "        # 4. Consensus & Fade\n",
    "        df['pick_norm'] = df['pick_value'].astype(str).str.lower().str.strip()\n",
    "        df['consensus_count'] = df.groupby(['pick_date', 'league_name', 'pick_norm'])['capper_id'].transform('count')\n",
    "        df['market_volume'] = df.groupby(['pick_date', 'league_name'])['capper_id'].transform('count')\n",
    "        df['consensus_pct'] = df['consensus_count'] / (df['market_volume'] + 1)\n",
    "        df['fade_score'] = (1 - df['consensus_pct']) * df['decimal_odds']\n",
    "\n",
    "        # 5. Momentum Logic\n",
    "        MOMENTUM_SPORTS = ['NBA', 'NCAAB', 'NHL', 'UFC']\n",
    "        df['is_momentum_sport'] = df['league_name'].isin(MOMENTUM_SPORTS).astype(int)\n",
    "        df['x_valid_hotness'] = df['raw_hotness'] * df['is_momentum_sport']\n",
    "\n",
    "        # 6. League ROI\n",
    "        df = df.sort_values('pick_date')\n",
    "        df['league_rolling_roi'] = df.groupby('league_name')['profit_units']\\\n",
    "            .transform(lambda x: x.rolling(window=200, min_periods=20).mean().shift(1)).fillna(0)\n",
    "\n",
    "        # 7. Probability Features\n",
    "        df['implied_prob'] = 1 / df['decimal_odds']\n",
    "        if 'bet_type_name' in df.columns:\n",
    "            df['bet_type_code'] = df['bet_type_name'].astype('category').cat.codes\n",
    "        else:\n",
    "            df['bet_type_code'] = 0\n",
    "            \n",
    "        return df\n",
    "\n",
    "engineer = FeatureEngineer(raw_data)\n",
    "processed_data = engineer.process_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optimizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING V3 PORTFOLIO OPTIMIZATION...\n",
      "   ‚Ä¢ Training Model on 38708 rows...\n",
      "\n",
      "üèÜ TOP 5 CONFIGURATIONS:\n",
      "   Min_Exp  Min_Edge  Daily_Cap        ROI      Profit  Bets   Bets/Day\n",
      "6       10      0.08          5  49.056380   82.905281   169   4.970588\n",
      "0       10      0.03          5  48.179577   81.905281   170   5.000000\n",
      "3       10      0.05          5  48.179577   81.905281   170   5.000000\n",
      "8       10      0.08         15  45.290740  217.848457   481  14.147059\n",
      "2       10      0.03         15  42.808920  216.613135   506  14.882353\n",
      "\n",
      "‚úÖ CHOSEN CONFIG: {'Min_Exp': 10, 'Min_Edge': 0.08, 'Daily_Cap': 5}\n",
      "   ‚Ä¢ ROI: 49.06%\n",
      "   ‚Ä¢ Total Profit: 82.91u\n",
      "   ‚Ä¢ Bets/Day: 5.0\n",
      "üìÅ Model and Config saved to models/v3_obsidian.pkl and models/v3_config.json\n"
     ]
    }
   ],
   "source": [
    "class StrategyOptimizerV3:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.features = [\n",
    "            'acc_7d', 'roi_7d', 'vol_7d', 'acc_30d', 'roi_30d', 'vol_30d',\n",
    "            'capper_experience', 'consensus_count', 'implied_prob', 'bet_type_code',\n",
    "            'raw_hotness', 'streak_entering_game', \n",
    "            'league_rolling_roi', 'fade_score',\n",
    "            'is_momentum_sport', 'x_valid_hotness'\n",
    "        ]\n",
    "        self.CORE_LEAGUES = ['NBA', 'NCAAB', 'NFL', 'NCAAF', 'NHL', 'UFC']\n",
    "\n",
    "    def run_optimization(self):\n",
    "        print(\"üöÄ STARTING V3 PORTFOLIO OPTIMIZATION...\")\n",
    "        \n",
    "        # 1. Filter Data (Core Leagues)\n",
    "        df_clean = self.df[self.df['outcome'].isin([0.0, 1.0])].copy()\n",
    "        # Note: We keep all leagues for now to be safe, or filter to core\n",
    "        df_clean = df_clean[df_clean['league_name'].isin(self.CORE_LEAGUES)]\n",
    "        df_clean = df_clean.dropna(subset=self.features).sort_values('pick_date')\n",
    "        \n",
    "        # 2. Train Model\n",
    "        split = int(len(df_clean) * 0.75) # 75% train to get more recent test data\n",
    "        train_df = df_clean.iloc[:split]\n",
    "        val_df = df_clean.iloc[split:].copy()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Training Model on {len(train_df)} rows...\")\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=1200, learning_rate=0.015, max_depth=5, \n",
    "            n_jobs=16, tree_method='hist', random_state=42,\n",
    "            early_stopping_rounds=50, subsample=0.8, colsample_bytree=0.8\n",
    "        )\n",
    "        model.fit(\n",
    "            train_df[self.features], train_df['outcome'].astype(int),\n",
    "            eval_set=[(val_df[self.features], val_df['outcome'].astype(int))],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Save Model\n",
    "        joblib.dump(model, '../models/v3_obsidian.pkl')\n",
    "        \n",
    "        # Predictions\n",
    "        val_df['prob'] = model.predict_proba(val_df[self.features])[:, 1]\n",
    "        val_df['edge'] = val_df['prob'] - val_df['implied_prob']\n",
    "        \n",
    "        # 3. GLOBAL GRID SEARCH (Portfolio Builder)\n",
    "        # We test stricter floors to ensure quality > quantity\n",
    "        edge_floors = [0.03, 0.05, 0.08] \n",
    "        exp_floors = [10]\n",
    "        daily_caps = [5, 10, 15] # Top N picks per day\n",
    "        \n",
    "        results = []\n",
    "        best_roi = -999\n",
    "        best_df = None\n",
    "        \n",
    "        for min_exp in exp_floors:\n",
    "            for min_edge in edge_floors:\n",
    "                for cap in daily_caps:\n",
    "                    \n",
    "                    # 1. Base Filter\n",
    "                    candidates = val_df[\n",
    "                        (val_df['capper_experience'] >= min_exp) &\n",
    "                        (val_df['edge'] >= min_edge) &\n",
    "                        (val_df['decimal_odds'] >= 1.70) # Min Odds Floor (-143)\n",
    "                    ].copy()\n",
    "                    \n",
    "                    if candidates.empty: continue\n",
    "\n",
    "                    # 2. Portfolio Construction (Top N Daily)\n",
    "                    # Rank by Raw Edge (Value)\n",
    "                    candidates = candidates.sort_values(['pick_date', 'edge'], ascending=[True, False])\n",
    "                    \n",
    "                    # Take top 'cap' picks per day across ALL leagues\n",
    "                    portfolio = candidates.groupby('pick_date').head(cap)\n",
    "                    \n",
    "                    # 3. Calculate Stats\n",
    "                    profit = portfolio['profit_units'].sum()\n",
    "                    volume = len(portfolio)\n",
    "                    roi = (profit / volume) * 100 if volume > 0 else 0\n",
    "                    unique_days = portfolio['pick_date'].nunique()\n",
    "                    bets_per_day = volume / unique_days if unique_days > 0 else 0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Min_Exp': min_exp, 'Min_Edge': min_edge, 'Daily_Cap': cap,\n",
    "                        'ROI': roi, 'Profit': profit, 'Bets': volume, 'Bets/Day': bets_per_day\n",
    "                    })\n",
    "                    \n",
    "                    if roi > best_roi and volume > 50 and bets_per_day >= 3:\n",
    "                        best_roi = roi\n",
    "                        best_config = {'Min_Exp': min_exp, 'Min_Edge': min_edge, 'Daily_Cap': cap}\n",
    "                        best_df = portfolio\n",
    "\n",
    "        # Result Display\n",
    "        res_df = pd.DataFrame(results).sort_values('ROI', ascending=False)\n",
    "        print(\"\\nüèÜ TOP 5 CONFIGURATIONS:\")\n",
    "        print(res_df.head(5))\n",
    "        \n",
    "        if best_df is not None:\n",
    "            print(f\"\\n‚úÖ CHOSEN CONFIG: {best_config}\")\n",
    "            print(f\"   ‚Ä¢ ROI: {best_roi:.2f}%\")\n",
    "            print(f\"   ‚Ä¢ Total Profit: {best_df['profit_units'].sum():.2f}u\")\n",
    "            print(f\"   ‚Ä¢ Bets/Day: {len(best_df)/best_df['pick_date'].nunique():.1f}\")\n",
    "            \n",
    "            # Save Config for Inference\n",
    "            import json\n",
    "            config_save = {\n",
    "                \"Min_Exp\": int(best_config['Min_Exp']),\n",
    "                \"Min_Edge\": float(best_config['Min_Edge']),\n",
    "                \"Min_Odds\": 1.70,\n",
    "                \"Daily_Cap\": int(best_config['Daily_Cap']),\n",
    "                \"ROI\": float(best_roi),\n",
    "                \"Profit\": float(best_df['profit_units'].sum()),\n",
    "                \"Bets\": int(len(best_df)),\n",
    "                \"Bets/Day\": float(len(best_df)/best_df['pick_date'].nunique())\n",
    "            }\n",
    "            with open('../models/v3_config.json', 'w') as f:\n",
    "                json.dump(config_save, f, indent=2)\n",
    "\n",
    "            print(\"üìÅ Model and Config saved to models/v3_obsidian.pkl and models/v3_config.json\")\n",
    "\n",
    "optimizer = StrategyOptimizerV3(processed_data)\n",
    "optimizer.run_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a38cd6-c5eb-435c-9c3c-9d0002f60252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
